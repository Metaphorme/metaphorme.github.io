<!doctype html><html lang=zh-cn><head><meta charset=utf-8><meta name=viewport content="width=device-width"><script type=application/javascript src=https://diazepam.cc/js/theme-mode.js></script><link rel=stylesheet href=https://diazepam.cc/css/frameworks.min.css><link rel=stylesheet href=https://diazepam.cc/css/github.min.css><link rel=stylesheet href=https://diazepam.cc/css/github-style.css><link rel=stylesheet href=https://diazepam.cc/css/light.css><link rel=stylesheet href=https://diazepam.cc/css/dark.css><link rel=stylesheet href=https://diazepam.cc/css/show_picture.css><link rel=stylesheet href=https://diazepam.cc/css/syntax.css><title>多种深度学习优化器最优化梯度更新路径图的绘制 - Diazepam's Zone</title><link rel=icon type=image/x-icon href=/images/dna.png><meta name=theme-color content="#1e2327"><meta name=description content="多种深度学习优化器最优化梯度更新路径图的绘制。
"><meta name=keywords content='blog,google analytics'><meta name=robots content="noodp"><link rel=canonical href=https://diazepam.cc/post/draw-optimal-gradient-update-path-graph/><meta name=twitter:card content="summary"><meta name=twitter:title content="多种深度学习优化器最优化梯度更新路径图的绘制 - Diazepam's Zone"><meta name=twitter:description content="多种深度学习优化器最优化梯度更新路径图的绘制。
"><meta name=twitter:site content="https://diazepam.cc/"><meta name=twitter:creator content="iDiazepam"><meta name=twitter:image content="https://diazepam.cc/"><meta property="og:type" content="article"><meta property="og:title" content="多种深度学习优化器最优化梯度更新路径图的绘制 - Diazepam's Zone"><meta property="og:description" content="多种深度学习优化器最优化梯度更新路径图的绘制。
"><meta property="og:url" content="https://diazepam.cc/post/draw-optimal-gradient-update-path-graph/"><meta property="og:site_name" content="多种深度学习优化器最优化梯度更新路径图的绘制"><meta property="og:image" content="https://diazepam.cc/"><meta property="og:image:width" content="2048"><meta property="og:image:height" content="1024"><meta property="article:published_time" content="2023-04-28 15:00:00 +0800 +0800"></head><body><div style=position:relative><header class="Header js-details-container Details px-3 px-md-4 px-lg-5 flex-wrap flex-md-nowrap open Details--on"><div class="Header-item mobile-none" style=margin-top:-4px;margin-bottom:-4px><a class=Header-link href=https://diazepam.cc/><svg class="octicon" height="32" viewBox="0 0 16 16" width="32"><path fill-rule="evenodd" d="M8 0C3.58.0.0 3.58.0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38.0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95.0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12.0.0.67-.21 2.2.82.64-.18 1.32-.27 2-.27s1.36.09 2 .27c1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15.0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48.0 1.07-.01 1.93-.01 2.2.0.21.15.46.55.38A8.013 8.013.0 0016 8c0-4.42-3.58-8-8-8z"/></svg></a></div><div class="Header-item d-md-none"><button class="Header-link btn-link js-details-target" type=button onclick='document.querySelector("#header-search").style.display=document.querySelector("#header-search").style.display=="none"?"block":"none"'>
<svg height="24" class="octicon octicon-three-bars" viewBox="0 0 16 16" width="24"><path fill-rule="evenodd" d="M1 2.75A.75.75.0 011.75 2h12.5a.75.75.0 110 1.5H1.75A.75.75.0 011 2.75zm0 5A.75.75.0 011.75 7h12.5a.75.75.0 110 1.5H1.75A.75.75.0 011 7.75zM1.75 12a.75.75.0 100 1.5h12.5a.75.75.0 100-1.5H1.75z"/></svg></button></div><div style=display:none id=header-search class="Header-item Header-item--full flex-column flex-md-row width-full flex-order-2 flex-md-order-none mr-0 mr-md-3 mt-3 mt-md-0 Details-content--hidden-not-important d-md-flex"><div class="Header-search header-search flex-auto js-site-search position-relative flex-self-stretch flex-md-self-auto mb-3 mb-md-0 mr-0 mr-md-3 scoped-search site-scoped-search js-jump-to"><div class=position-relative><form target=_blank action=https://www.google.com/search accept-charset=UTF-8 method=get autocomplete=off><label class="Header-search-label form-control input-sm header-search-wrapper p-0 js-chromeless-input-container header-search-wrapper-jump-to position-relative d-flex flex-justify-between flex-items-center"><input type=text class="Header-search-input form-control input-sm header-search-input jump-to-field js-jump-to-field js-site-search-focus js-site-search-field is-clearable" name=q placeholder=Search autocomplete=off>
<input type=hidden name=q value=site:https://diazepam.cc/></label></form></div></div></div><div class="Header-item Header-item--full flex-justify-center d-md-none position-relative"><a class=Header-link href=https://diazepam.cc/><svg class="octicon octicon-mark-github v-align-middle" height="32" viewBox="0 0 16 16" width="32"><path fill-rule="evenodd" d="M8 0C3.58.0.0 3.58.0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38.0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95.0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12.0.0.67-.21 2.2.82.64-.18 1.32-.27 2-.27s1.36.09 2 .27c1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15.0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48.0 1.07-.01 1.93-.01 2.2.0.21.15.46.55.38A8.013 8.013.0 0016 8c0-4.42-3.58-8-8-8z"/></svg></a></div><div class=Header-item style=margin-right:0><a href=javascript:void(0) class="Header-link no-select" onclick=switchTheme()><svg style="fill:var(--color-profile-color-modes-toggle-moon)" class="no-select" viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" clip-rule="evenodd" d="M4.52208 7.71754c3.05612.0 5.53362-2.47748 5.53362-5.5336C10.0557 1.93498 10.0392 1.68986 10.0074 1.44961 9.95801 1.07727 10.3495.771159 10.6474.99992c1.4679 1.12724 2.4141 2.90007 2.4141 4.89391.0 3.40575-2.7609 6.16667-6.16665 6.16667-2.94151.0-5.40199-2.0595-6.018122-4.81523C.794841 6.87902 1.23668 6.65289 1.55321 6.85451 2.41106 7.40095 3.4296 7.71754 4.52208 7.71754z"/></svg></a></div></header></div><div><main><div class="gisthead pagehead bg-gray-light pb-0 pt-3 mb-4"><div class=px-0><div class="mb-3 d-flex px-3 px-md-3 px-lg-5"><div class="flex-auto min-width-0 width-fit mr-3"><div class=d-flex><div class="d-none d-md-block"><a class="avatar mr-2 flex-shrink-0" href=https://diazepam.cc/><img class=avatar-user src=https://diazepam.cc/images/avatar.png width=32 height=32></a></div><div class="d-flex flex-column"><h1 class="break-word f3 text-normal mb-md-0 mb-1"><span class=author><a href=https://diazepam.cc/>Diazepam</a></span><span class=path-divider>/</span><strong class="css-truncate-target mr-1" style=max-width:410px><a href=https://diazepam.cc/post/draw-optimal-gradient-update-path-graph/>多种深度学习优化器最优化梯度更新路径图的绘制</a></strong></h1><div class="note m-0">Created <relative-time datetime="Fri, 28 Apr 2023 15:00:00 +0800" class=no-wrap>Fri, 28 Apr 2023 15:00:00 +0800</relative-time>
<span class=file-info-divider></span>
Modified <relative-time datetime="Fri, 15 Aug 2025 06:44:05 +0000" class=no-wrap>Fri, 15 Aug 2025 06:44:05 +0000</relative-time></div></div></div></div></div></div></div><div class="container-lg px-3 new-discussion-timeline"><div class="repository-content gist-content"><div><div class="js-gist-file-update-container js-task-list-container file-box"><div id=file-pytest class="file my-2"><div id=post-header class="file-header d-flex flex-md-items-center flex-items-start sticky-header" style=z-index:2><div class="file-info d-flex flex-md-items-center flex-items-start flex-order-1 flex-auto"><div class="text-mono f6 flex-auto pr-3 flex-order-2 flex-md-order-1 mt-2 mt-md-0"><summary id=toc-toggle onclick=clickToc() class="btn btn-octicon m-0 mr-2 p-2"><svg aria-hidden="true" viewBox="0 0 16 16" height="16" width="16" class="octicon octicon-list-unordered"><path fill-rule="evenodd" d="M2 4a1 1 0 100-2 1 1 0 000 2zm3.75-1.5a.75.75.0 000 1.5h8.5a.75.75.0 000-1.5h-8.5zm0 5a.75.75.0 000 1.5h8.5a.75.75.0 000-1.5h-8.5zm0 5a.75.75.0 000 1.5h8.5a.75.75.0 000-1.5h-8.5zM3 8A1 1 0 111 8a1 1 0 012 0zm-1 6a1 1 0 100-2 1 1 0 000 2z"/></svg></summary><details-menu class=SelectMenu id=toc-details style="display: none;"><div class="SelectMenu-modal rounded-3 mt-1" style=max-height:340px><div class="SelectMenu-list SelectMenu-list--borderless p-2" style=overscroll-behavior:contain id=toc-list></div></div></details-menu>1715 Words</div><div class="file-actions flex-order-2 pt-0"><a class="muted-link mr-3" href=/tags/deep-learning><svg class="octicon octicon-tag" viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M2.5 7.775V2.75a.25.25.0 01.25-.25h5.025a.25.25.0 01.177.073l6.25 6.25a.25.25.0 010 .354l-5.025 5.025a.25.25.0 01-.354.0l-6.25-6.25A.25.25.0 012.5 7.775zm-1.5.0V2.75C1 1.784 1.784 1 2.75 1h5.025c.464.0.91.184 1.238.513l6.25 6.25a1.75 1.75.0 010 2.474l-5.026 5.026a1.75 1.75.0 01-2.474.0l-6.25-6.25A1.75 1.75.0 011 7.775zM6 5a1 1 0 100 2 1 1 0 000-2z"/></svg>
deep learning
</a><a class="muted-link mr-3" href=/tags/idiazepam><svg class="octicon octicon-tag" viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M2.5 7.775V2.75a.25.25.0 01.25-.25h5.025a.25.25.0 01.177.073l6.25 6.25a.25.25.0 010 .354l-5.025 5.025a.25.25.0 01-.354.0l-6.25-6.25A.25.25.0 012.5 7.775zm-1.5.0V2.75C1 1.784 1.784 1 2.75 1h5.025c.464.0.91.184 1.238.513l6.25 6.25a1.75 1.75.0 010 2.474l-5.026 5.026a1.75 1.75.0 01-2.474.0l-6.25-6.25A1.75 1.75.0 011 7.775zM6 5a1 1 0 100 2 1 1 0 000-2z"/></svg>
iDiazepam</a></div></div></div><div class="Box-body px-5 pb-5" style=z-index:1><article class="markdown-body entry-content container-lg"><p>多种深度学习优化器最优化梯度更新路径图的绘制。</p><p>本文对深度学习中SGD、Momentum、AdaGrad、Adam四种优化器进行了最优化梯度更新路径图的对比绘制，将介绍Python和MATLAB两种代码实现方式。</p><h1 id=python实现方法>PYTHON实现方法</h1><ol><li>引入需要的库</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> sys<span style=color:#f92672>,</span> os
</span></span><span style=display:flex><span>sys<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>append(os<span style=color:#f92672>.</span>pardir)  <span style=color:#75715e># 父目录file设置</span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> matplotlib.pyplot <span style=color:#66d9ef>as</span> plt
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> collections <span style=color:#f92672>import</span> OrderedDict
</span></span></code></pre></div><p><code>OrderedDict</code>可以维护一个有序的字典。与普通的字典不同，<code>OrderedDict</code> 会在添加元素时保持元素的顺序，这使得在需要按照键的顺序遍历字典时非常有用。</p><ol><li>SGD优化器类的定义</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># SGD（随机梯度下降）优化器类</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>SGD</span>:
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># 初始化函数，参数lr表示学习率，默认值为0.01</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__init__</span>(self, lr<span style=color:#f92672>=</span><span style=color:#ae81ff>0.01</span>):
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>lr <span style=color:#f92672>=</span> lr
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># 更新函数，参数params表示待优化参数字典，grads表示损失函数关于参数的梯度字典</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>update</span>(self, params, grads):
</span></span><span style=display:flex><span>        <span style=color:#75715e># 遍历待优化参数字典中的所有键</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> key <span style=color:#f92672>in</span> params<span style=color:#f92672>.</span>keys():
</span></span><span style=display:flex><span>            <span style=color:#75715e># 根据随机梯度下降更新公式，更新参数</span>
</span></span><span style=display:flex><span>            params[key] <span style=color:#f92672>-=</span> self<span style=color:#f92672>.</span>lr <span style=color:#f92672>*</span> grads[key]
</span></span></code></pre></div><ol><li>Momentum优化器类的定义</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Momentum（动量法）优化器类</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>Momentum</span>:
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># 初始化函数，参数lr表示学习率，默认值为0.01，momentum表示动量系数，默认值为0.9</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__init__</span>(self, lr<span style=color:#f92672>=</span><span style=color:#ae81ff>0.01</span>, momentum<span style=color:#f92672>=</span><span style=color:#ae81ff>0.9</span>):
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>lr <span style=color:#f92672>=</span> lr
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>momentum <span style=color:#f92672>=</span> momentum
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>v <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>  <span style=color:#75715e># 初始化动量变量v为空字典</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># 更新函数，参数params表示待优化参数字典，grads表示损失函数关于参数的梯度字典</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>update</span>(self, params, grads):
</span></span><span style=display:flex><span>        <span style=color:#75715e># 如果动量变量v为空，则初始化为与params具有相同形状的全零数组</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> self<span style=color:#f92672>.</span>v <span style=color:#f92672>is</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>            self<span style=color:#f92672>.</span>v <span style=color:#f92672>=</span> {}
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>for</span> key, val <span style=color:#f92672>in</span> params<span style=color:#f92672>.</span>items():
</span></span><span style=display:flex><span>                self<span style=color:#f92672>.</span>v[key] <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>zeros_like(val)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 遍历待优化参数字典中的所有键</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> key <span style=color:#f92672>in</span> params<span style=color:#f92672>.</span>keys():
</span></span><span style=display:flex><span>            <span style=color:#75715e># 根据动量法更新公式，计算动量变量v和参数更新值，并更新params字典中的参数</span>
</span></span><span style=display:flex><span>            self<span style=color:#f92672>.</span>v[key] <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>momentum <span style=color:#f92672>*</span> self<span style=color:#f92672>.</span>v[key] <span style=color:#f92672>-</span> self<span style=color:#f92672>.</span>lr <span style=color:#f92672>*</span> grads[key]
</span></span><span style=display:flex><span>            params[key] <span style=color:#f92672>+=</span> self<span style=color:#f92672>.</span>v[key]
</span></span></code></pre></div><ol><li>AdaGrad优化器类的定义</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># AdaGrad（自适应学习率法）优化器类</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>AdaGrad</span>:
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># 初始化函数，参数lr表示学习率，默认值为0.01</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__init__</span>(self, lr<span style=color:#f92672>=</span><span style=color:#ae81ff>0.01</span>):
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>lr <span style=color:#f92672>=</span> lr
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>h <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>  <span style=color:#75715e># 初始化参数平方梯度和h为空字典</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># 更新函数，参数params表示待优化参数字典，grads表示损失函数关于参数的梯度字典</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>update</span>(self, params, grads):
</span></span><span style=display:flex><span>        <span style=color:#75715e># 如果参数平方梯度和h为空，则初始化为与params具有相同形状的全零数组</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> self<span style=color:#f92672>.</span>h <span style=color:#f92672>is</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>            self<span style=color:#f92672>.</span>h <span style=color:#f92672>=</span> {}
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>for</span> key, val <span style=color:#f92672>in</span> params<span style=color:#f92672>.</span>items():
</span></span><span style=display:flex><span>                self<span style=color:#f92672>.</span>h[key] <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>zeros_like(val)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 遍历待优化参数字典中的所有键</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> key <span style=color:#f92672>in</span> params<span style=color:#f92672>.</span>keys():
</span></span><span style=display:flex><span>            <span style=color:#75715e># 根据AdaGrad更新公式，计算参数平方梯度和h，更新params字典中的参数</span>
</span></span><span style=display:flex><span>            self<span style=color:#f92672>.</span>h[key] <span style=color:#f92672>+=</span> grads[key] <span style=color:#f92672>*</span> grads[key]
</span></span><span style=display:flex><span>            params[key] <span style=color:#f92672>-=</span> self<span style=color:#f92672>.</span>lr <span style=color:#f92672>*</span> grads[key] <span style=color:#f92672>/</span> (np<span style=color:#f92672>.</span>sqrt(self<span style=color:#f92672>.</span>h[key]) <span style=color:#f92672>+</span> <span style=color:#ae81ff>1e-7</span>)
</span></span></code></pre></div><ol><li>Adam优化器类的定义</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>Adam</span>:
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>__init__</span>(self, lr<span style=color:#f92672>=</span><span style=color:#ae81ff>0.001</span>, beta1<span style=color:#f92672>=</span><span style=color:#ae81ff>0.9</span>, beta2<span style=color:#f92672>=</span><span style=color:#ae81ff>0.999</span>):
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>lr <span style=color:#f92672>=</span> lr                      <span style=color:#75715e># 学习率</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>beta1 <span style=color:#f92672>=</span> beta1                <span style=color:#75715e># 一阶矩的指数衰减率</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>beta2 <span style=color:#f92672>=</span> beta2                <span style=color:#75715e># 二阶矩的指数衰减率</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>iter <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>                     <span style=color:#75715e># 迭代次数</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>m <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>                     <span style=color:#75715e># 一阶矩的变量</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>v <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>                     <span style=color:#75715e># 二阶矩的变量</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>update</span>(self, params, grads):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> self<span style=color:#f92672>.</span>m <span style=color:#f92672>is</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>            self<span style=color:#f92672>.</span>m, self<span style=color:#f92672>.</span>v <span style=color:#f92672>=</span> {}, {}
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>for</span> key, val <span style=color:#f92672>in</span> params<span style=color:#f92672>.</span>items():
</span></span><span style=display:flex><span>                self<span style=color:#f92672>.</span>m[key] <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>zeros_like(val)
</span></span><span style=display:flex><span>                self<span style=color:#f92672>.</span>v[key] <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>zeros_like(val)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>iter <span style=color:#f92672>+=</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>        lr_t <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>lr <span style=color:#f92672>*</span> np<span style=color:#f92672>.</span>sqrt(<span style=color:#ae81ff>1.0</span> <span style=color:#f92672>-</span> self<span style=color:#f92672>.</span>beta2 <span style=color:#f92672>**</span> self<span style=color:#f92672>.</span>iter) <span style=color:#f92672>/</span> (<span style=color:#ae81ff>1.0</span> <span style=color:#f92672>-</span> self<span style=color:#f92672>.</span>beta1 <span style=color:#f92672>**</span> self<span style=color:#f92672>.</span>iter)  <span style=color:#75715e># 计算修正后的学习率</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> key <span style=color:#f92672>in</span> params<span style=color:#f92672>.</span>keys():
</span></span><span style=display:flex><span>            self<span style=color:#f92672>.</span>m[key] <span style=color:#f92672>+=</span> (<span style=color:#ae81ff>1</span> <span style=color:#f92672>-</span> self<span style=color:#f92672>.</span>beta1) <span style=color:#f92672>*</span> (grads[key] <span style=color:#f92672>-</span> self<span style=color:#f92672>.</span>m[key])  <span style=color:#75715e># 计算一阶矩的指数加权平均</span>
</span></span><span style=display:flex><span>            self<span style=color:#f92672>.</span>v[key] <span style=color:#f92672>+=</span> (<span style=color:#ae81ff>1</span> <span style=color:#f92672>-</span> self<span style=color:#f92672>.</span>beta2) <span style=color:#f92672>*</span> (grads[key] <span style=color:#f92672>**</span> <span style=color:#ae81ff>2</span> <span style=color:#f92672>-</span> self<span style=color:#f92672>.</span>v[key])  <span style=color:#75715e># 计算二阶矩的指数加权平均</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            params[key] <span style=color:#f92672>-=</span> lr_t <span style=color:#f92672>*</span> self<span style=color:#f92672>.</span>m[key] <span style=color:#f92672>/</span> (np<span style=color:#f92672>.</span>sqrt(self<span style=color:#f92672>.</span>v[key]) <span style=color:#f92672>+</span> <span style=color:#ae81ff>1e-7</span>)  <span style=color:#75715e># 更新参数</span>
</span></span></code></pre></div><ol><li>作图</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>f</span>(x, y):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> x <span style=color:#f92672>**</span> <span style=color:#ae81ff>2</span> <span style=color:#f92672>/</span> <span style=color:#ae81ff>20.0</span> <span style=color:#f92672>+</span> y <span style=color:#f92672>**</span> <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>df</span>(x, y):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> x <span style=color:#f92672>/</span> <span style=color:#ae81ff>10.0</span>, <span style=color:#ae81ff>2.0</span> <span style=color:#f92672>*</span> y
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>init_pos <span style=color:#f92672>=</span> (<span style=color:#f92672>-</span><span style=color:#ae81ff>7.0</span>, <span style=color:#ae81ff>2.0</span>)
</span></span><span style=display:flex><span>params <span style=color:#f92672>=</span> {}
</span></span><span style=display:flex><span>params[<span style=color:#e6db74>&#39;x&#39;</span>], params[<span style=color:#e6db74>&#39;y&#39;</span>] <span style=color:#f92672>=</span> init_pos[<span style=color:#ae81ff>0</span>], init_pos[<span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>grads <span style=color:#f92672>=</span> {}
</span></span><span style=display:flex><span>grads[<span style=color:#e6db74>&#39;x&#39;</span>], grads[<span style=color:#e6db74>&#39;y&#39;</span>] <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>optimizers <span style=color:#f92672>=</span> OrderedDict()
</span></span><span style=display:flex><span>optimizers[<span style=color:#e6db74>&#34;SGD&#34;</span>] <span style=color:#f92672>=</span> SGD(lr<span style=color:#f92672>=</span><span style=color:#ae81ff>0.95</span>)
</span></span><span style=display:flex><span>optimizers[<span style=color:#e6db74>&#34;Momentum&#34;</span>] <span style=color:#f92672>=</span> Momentum(lr<span style=color:#f92672>=</span><span style=color:#ae81ff>0.1</span>)
</span></span><span style=display:flex><span>optimizers[<span style=color:#e6db74>&#34;AdaGrad&#34;</span>] <span style=color:#f92672>=</span> AdaGrad(lr<span style=color:#f92672>=</span><span style=color:#ae81ff>1.5</span>)
</span></span><span style=display:flex><span>optimizers[<span style=color:#e6db74>&#34;Adam&#34;</span>] <span style=color:#f92672>=</span> Adam(lr<span style=color:#f92672>=</span><span style=color:#ae81ff>0.3</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>idx <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>figure(figsize<span style=color:#f92672>=</span>(<span style=color:#ae81ff>12</span>, <span style=color:#ae81ff>10</span>))
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> key <span style=color:#f92672>in</span> optimizers:
</span></span><span style=display:flex><span>    optimizer <span style=color:#f92672>=</span> optimizers[key]
</span></span><span style=display:flex><span>    x_history <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>    y_history <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>    params[<span style=color:#e6db74>&#39;x&#39;</span>], params[<span style=color:#e6db74>&#39;y&#39;</span>] <span style=color:#f92672>=</span> init_pos[<span style=color:#ae81ff>0</span>], init_pos[<span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>30</span>):
</span></span><span style=display:flex><span>        x_history<span style=color:#f92672>.</span>append(params[<span style=color:#e6db74>&#39;x&#39;</span>])
</span></span><span style=display:flex><span>        y_history<span style=color:#f92672>.</span>append(params[<span style=color:#e6db74>&#39;y&#39;</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        grads[<span style=color:#e6db74>&#39;x&#39;</span>], grads[<span style=color:#e6db74>&#39;y&#39;</span>] <span style=color:#f92672>=</span> df(params[<span style=color:#e6db74>&#39;x&#39;</span>], params[<span style=color:#e6db74>&#39;y&#39;</span>])
</span></span><span style=display:flex><span>        optimizer<span style=color:#f92672>.</span>update(params, grads)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    x <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>arange(<span style=color:#f92672>-</span><span style=color:#ae81ff>10</span>, <span style=color:#ae81ff>10</span>, <span style=color:#ae81ff>0.01</span>)
</span></span><span style=display:flex><span>    y <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>arange(<span style=color:#f92672>-</span><span style=color:#ae81ff>5</span>, <span style=color:#ae81ff>5</span>, <span style=color:#ae81ff>0.01</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    X, Y <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>meshgrid(x, y)
</span></span><span style=display:flex><span>    Z <span style=color:#f92672>=</span> f(X, Y)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># 外围线颜色生成</span>
</span></span><span style=display:flex><span>    mask <span style=color:#f92672>=</span> Z <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>5</span>
</span></span><span style=display:flex><span>    Z[mask] <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># 作图</span>
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>subplot(<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>2</span>, idx)
</span></span><span style=display:flex><span>    idx <span style=color:#f92672>+=</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>plot(x_history, y_history, <span style=color:#e6db74>&#39;o-&#39;</span>, color<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;#553BF5&#34;</span>)
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>contour(X, Y, Z)
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>ylim(<span style=color:#f92672>-</span><span style=color:#ae81ff>10</span>, <span style=color:#ae81ff>10</span>)
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>xlim(<span style=color:#f92672>-</span><span style=color:#ae81ff>10</span>, <span style=color:#ae81ff>10</span>)
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>plot(<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#e6db74>&#39;+&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>title(key)
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>xlabel(<span style=color:#e6db74>&#34;x&#34;</span>)
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>ylabel(<span style=color:#e6db74>&#34;y&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>show()
</span></span></code></pre></div><ol><li>作图效果</li></ol><p><img src=image_oqwCJgHtTx.png alt></p><h1 id=等效的matlab代码实现>等效的MATLAB代码实现</h1><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-matlab data-lang=matlab><span style=display:flex><span><span style=color:#66d9ef>function</span> <span style=color:#a6e22e>plot_optimizer</span>(optimizer, num_iterations, learning_rate)
</span></span><span style=display:flex><span><span style=color:#75715e>% optimizer: 优化器名称，如 &#39;SGD&#39;, &#39;Momentum&#39;, &#39;AdaGrad&#39;, &#39;Adam&#39;</span>
</span></span><span style=display:flex><span><span style=color:#75715e>% num_iterations: 迭代次数</span>
</span></span><span style=display:flex><span><span style=color:#75715e>% learning_rate: 学习率</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>% 初始化变量</span>
</span></span><span style=display:flex><span>x = <span style=color:#f92672>-</span><span style=color:#ae81ff>5</span>:<span style=color:#ae81ff>0.1</span>:<span style=color:#ae81ff>5</span>; <span style=color:#75715e>% 横坐标</span>
</span></span><span style=display:flex><span>y = <span style=color:#f92672>-</span><span style=color:#ae81ff>5</span>:<span style=color:#ae81ff>0.1</span>:<span style=color:#ae81ff>5</span>; <span style=color:#75715e>% 纵坐标</span>
</span></span><span style=display:flex><span>z = zeros(length(x), length(y)); <span style=color:#75715e>% 损失函数值</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>% 计算损失函数值</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> i = <span style=color:#ae81ff>1</span>:length(x)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> j = <span style=color:#ae81ff>1</span>:length(y)
</span></span><span style=display:flex><span>        z(i,j) = loss_function(x(i), y(j)); <span style=color:#75715e>% 损失函数为 x^2 + y^2</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>end</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>end</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>% 初始化变量</span>
</span></span><span style=display:flex><span>params = [<span style=color:#f92672>-</span><span style=color:#ae81ff>4</span>; <span style=color:#ae81ff>4</span>]; <span style=color:#75715e>% 初始参数</span>
</span></span><span style=display:flex><span>velocity = [<span style=color:#ae81ff>0</span>; <span style=color:#ae81ff>0</span>]; <span style=color:#75715e>% 初始速度</span>
</span></span><span style=display:flex><span>grad_squared_sum = [<span style=color:#ae81ff>0</span>; <span style=color:#ae81ff>0</span>]; <span style=color:#75715e>% 初始梯度平方和</span>
</span></span><span style=display:flex><span>m = [<span style=color:#ae81ff>0</span>; <span style=color:#ae81ff>0</span>]; <span style=color:#75715e>% 初始一阶矩向量</span>
</span></span><span style=display:flex><span>v = [<span style=color:#ae81ff>0</span>; <span style=color:#ae81ff>0</span>]; <span style=color:#75715e>% 初始二阶矩向量</span>
</span></span><span style=display:flex><span>epsilon = <span style=color:#ae81ff>1e-8</span>; <span style=color:#75715e>% 防止除0错误</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>% 绘制等高线图</span>
</span></span><span style=display:flex><span>figure;
</span></span><span style=display:flex><span>contour(x,y,z,<span style=color:#ae81ff>40</span>);
</span></span><span style=display:flex><span>colorbar;
</span></span><span style=display:flex><span>hold on;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>% 迭代更新</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> i = <span style=color:#ae81ff>1</span>:num_iterations
</span></span><span style=display:flex><span>    gradient = compute_gradient(params); <span style=color:#75715e>% 计算梯度</span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e>% 根据优化器更新参数</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>switch</span> optimizer
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>case</span> <span style=color:#e6db74>&#39;SGD&#39;</span>
</span></span><span style=display:flex><span>            params = params <span style=color:#f92672>-</span> learning_rate <span style=color:#f92672>*</span> gradient;
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>case</span> <span style=color:#e6db74>&#39;Momentum&#39;</span>
</span></span><span style=display:flex><span>            velocity = <span style=color:#ae81ff>0.9</span> <span style=color:#f92672>*</span> velocity <span style=color:#f92672>+</span> <span style=color:#ae81ff>0.1</span> <span style=color:#f92672>*</span> gradient;
</span></span><span style=display:flex><span>            params = params <span style=color:#f92672>-</span> learning_rate <span style=color:#f92672>*</span> velocity;
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>case</span> <span style=color:#e6db74>&#39;AdaGrad&#39;</span>
</span></span><span style=display:flex><span>            grad_squared_sum = grad_squared_sum <span style=color:#f92672>+</span> gradient <span style=color:#f92672>.^</span> <span style=color:#ae81ff>2</span>;
</span></span><span style=display:flex><span>            adjusted_gradient = gradient <span style=color:#f92672>./</span> (sqrt(grad_squared_sum) <span style=color:#f92672>+</span> epsilon);
</span></span><span style=display:flex><span>            params = params <span style=color:#f92672>-</span> learning_rate <span style=color:#f92672>*</span> adjusted_gradient;
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>case</span> <span style=color:#e6db74>&#39;Adam&#39;</span>
</span></span><span style=display:flex><span>            m = <span style=color:#ae81ff>0.9</span> <span style=color:#f92672>*</span> m <span style=color:#f92672>+</span> <span style=color:#ae81ff>0.1</span> <span style=color:#f92672>*</span> gradient;
</span></span><span style=display:flex><span>            v = <span style=color:#ae81ff>0.999</span> <span style=color:#f92672>*</span> v <span style=color:#f92672>+</span> <span style=color:#ae81ff>0.001</span> <span style=color:#f92672>*</span> gradient <span style=color:#f92672>.^</span> <span style=color:#ae81ff>2</span>;
</span></span><span style=display:flex><span>            m_hat = m <span style=color:#f92672>/</span> (<span style=color:#ae81ff>1</span> <span style=color:#f92672>-</span> <span style=color:#ae81ff>0.9</span>^i);
</span></span><span style=display:flex><span>            v_hat = v <span style=color:#f92672>/</span> (<span style=color:#ae81ff>1</span> <span style=color:#f92672>-</span> <span style=color:#ae81ff>0.999</span>^i);
</span></span><span style=display:flex><span>            params = params <span style=color:#f92672>-</span> learning_rate <span style=color:#f92672>*</span> m_hat <span style=color:#f92672>./</span> (sqrt(v_hat) <span style=color:#f92672>+</span> epsilon);
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>end</span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e>% 绘制更新路径</span>
</span></span><span style=display:flex><span>    plot(params(<span style=color:#ae81ff>1</span>), params(<span style=color:#ae81ff>2</span>), <span style=color:#e6db74>&#39;rx&#39;</span>);
</span></span><span style=display:flex><span>    pause(<span style=color:#ae81ff>0.01</span>);
</span></span><span style=display:flex><span><span style=color:#66d9ef>end</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>end</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>function</span> loss = <span style=color:#a6e22e>loss_function</span>(x, y)
</span></span><span style=display:flex><span><span style=color:#75715e>% 损失函数为 x^2 + y^2</span>
</span></span><span style=display:flex><span>loss = x^<span style=color:#ae81ff>2</span> <span style=color:#f92672>+</span> y^<span style=color:#ae81ff>2</span>;
</span></span><span style=display:flex><span><span style=color:#66d9ef>end</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>function</span> gradient = <span style=color:#a6e22e>compute_gradient</span>(params)
</span></span><span style=display:flex><span><span style=color:#75715e>% 梯度为 (2x, 2y)</span>
</span></span><span style=display:flex><span>gradient = [<span style=color:#ae81ff>2</span><span style=color:#f92672>*</span>params(<span style=color:#ae81ff>1</span>); <span style=color:#ae81ff>2</span><span style=color:#f92672>*</span>params(<span style=color:#ae81ff>2</span>)];
</span></span><span style=display:flex><span><span style=color:#66d9ef>end</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>% 绘制SGD的梯度更新路径图</span>
</span></span><span style=display:flex><span>figure;
</span></span><span style=display:flex><span>plot_optimizer(<span style=color:#e6db74>&#39;SGD&#39;</span>, <span style=color:#ae81ff>100</span>, <span style=color:#ae81ff>0.1</span>);
</span></span><span style=display:flex><span>title(<span style=color:#e6db74>&#39;SGD&#39;</span>);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>% 绘制Momentum的梯度更新路径图</span>
</span></span><span style=display:flex><span>figure;
</span></span><span style=display:flex><span>plot_optimizer(<span style=color:#e6db74>&#39;Momentum&#39;</span>, <span style=color:#ae81ff>100</span>, <span style=color:#ae81ff>0.1</span>);
</span></span><span style=display:flex><span>title(<span style=color:#e6db74>&#39;Momentum&#39;</span>);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>% 绘制AdaGrad的梯度更新路径图</span>
</span></span><span style=display:flex><span>figure;
</span></span><span style=display:flex><span>plot_optimizer(<span style=color:#e6db74>&#39;AdaGrad&#39;</span>, <span style=color:#ae81ff>100</span>, <span style=color:#ae81ff>0.1</span>);
</span></span><span style=display:flex><span>title(<span style=color:#e6db74>&#39;AdaGrad&#39;</span>);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>% 绘制Adam的梯度更新路径图</span>
</span></span><span style=display:flex><span>figure;
</span></span><span style=display:flex><span>plot_optimizer(<span style=color:#e6db74>&#39;Adam&#39;</span>, <span style=color:#ae81ff>100</span>, <span style=color:#ae81ff>0.1</span>);
</span></span><span style=display:flex><span>title(<span style=color:#e6db74>&#39;Adam&#39;</span>);
</span></span></code></pre></div></article></div></div></div></div></div></div></main></div><script type=application/javascript src=https://diazepam.cc/js/toc.js></script><link rel=stylesheet href=https://diazepam.cc/css/toc.css><div id=gitalk-container class=gitalk-container></div><link rel=stylesheet href=https://diazepam.cc/css/gitalk.css><script src=https://diazepam.cc/js/gitalk.min.js></script><script>const gitalk=new Gitalk({clientID:"e634d9a28d7aa0a810f5",clientSecret:"7493396cb521167eaa0893b3058a1aba6baddcef",repo:"metaphorme.github.io",owner:"Metaphorme",admin:["Metaphorme"],id:eval("location.pathname"),distractionFreeMode:!1});(function(){gitalk.render("gitalk-container")})()</script><script type=text/javascript src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><div class="footer container-xl width-full p-responsive"><div class="position-relative d-flex flex-row-reverse flex-lg-row flex-wrap flex-lg-nowrap flex-justify-center flex-lg-justify-between flex-sm-items-center pt-6 pb-2 mt-6 f6 text-gray border-top border-gray-light"><a aria-label=Homepage title=GitHub class="footer-octicon d-none d-lg-block mr-lg-4" href=https://diazepam.cc/><svg height="24" class="octicon octicon-mark-github" viewBox="0 0 16 16" width="24"><path fill-rule="evenodd" d="M8 0C3.58.0.0 3.58.0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38.0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95.0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12.0.0.67-.21 2.2.82.64-.18 1.32-.27 2-.27s1.36.09 2 .27c1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15.0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48.0 1.07-.01 1.93-.01 2.2.0.21.15.46.55.38A8.013 8.013.0 0016 8c0-4.42-3.58-8-8-8z"/></svg></a><ul class="list-style-none d-flex flex-wrap col-12 flex-justify-center flex-lg-justify-between mb-2 mb-lg-0"><li class="mr-3 mr-lg-0">Theme by <a href=https://github.com/MeiK2333/github-style>github-style</a></li></ul></div><div class="d-flex flex-justify-center pb-6"><span class="f6 text-gray-light"></span></div></div></body><script type=application/javascript src=https://diazepam.cc/js/github-style.js></script></html>